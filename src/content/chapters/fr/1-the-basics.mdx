---
chapter: 1
title: Les “bases”
shortname: Bases
slug: les-bases
updatedAt: 2023-07-19T18:57:54.630Z
---

Le truc qui m'a constamment surpris en écrivant cet article, c'est à quel point les ordinateurs sont *simples*. C'est toujours compliqué pour moi de pas me prendre la tête, en m'attendant à quelque chose de plus compliqué ou abstrait que ça l'est en réalité! S'il y a une chose à vous enfoncer dans le crâne avant de continuer, c'est que les choses qui semblent simples le sont vraiment. Cette simplicité est très belle et parfois très, très ?????

Commençons par les bases sur le fonctionnement de votre ordinateur, au plus profond.

## Comment les ordinateurs sont architecturés

Le *processeur* (en anglais CPU, *central processing unit*) d'un ordinateur s'occupe de tous les calculs. C'est le big boss. Il commence à cravacher dès que vous allumez votre ordinateur, en exécutant instruction après instruction. 

Le premier processeur produit en masse était l'[Intel 4004](http://www.intel4004.com/), conçu à la fin des années 60 par un physicien et ingénieur Italien nommé Federico Faggin. Il utilisait une architecture 4-bit, à la différence des systèmes [64-bit]https://fr.wikipedia.org/wiki/Processeur_64_bits) qu'on utilise aujourd'hui, et était bien moins complexe que les processeurs modernes, mais une grande partie de sa simplicité est toujours d'actualité.

Les "instructions" exécutées par le CPU sont juste des données binaires : un octet ou deux pour représenter l'instruction qui s’exécute (l'opcode), suivi par les données nécessaires à son exécution. Ce qu'on appelle le *code machine* n'est rien d'autre qu'une série de ces instructions binaires à la suite. L'[assembleur](https://fr.wikipedia.org/wiki/Assembleur) est une syntaxe bien pratique pour lire et écrire du code machine. Elle est plus facile à lire et écrire pour des humains que des bits bruts; et est toujours compilée en code binaire que votre CPU saura lire.

<img src='/images/assembly-to-machine-code-translation.png' loading='eager' style='max-width: 400px;' alt='Un diagramme montrant comment le code machine est converti en assembleur, et à nouveau en code machine. Une flèche bi-directionnelle connecte trois exemples : du code machine (binaire) suivi par trois octets de nombres binaires, du code machine (hexadécimal) suivi par ces trois octets convertis en hexadécimal (0x83, 0xC3, 0x0A), puis le mot Assembleur suivi  de "add ebx, 10". L''assembleur et le code machine sont colorés pour expliciter le fait que chaque octet du code machine devient un mot de l''assembleur' width='935' height='505' />

> Une note : les instructions ne sont pas toujours représentées 1 pour 1 en code machine. Par exemple, `add eax, 512` devient `05 00 02 00 00`.
> 
> Le premier octet (`05`) qui veut exactement dire *ajouter le registre EAX à un un nombre 32-bits*. Les octets restants représentent 512 (`0x200`) en ordre [little-endian](https://fr.wikipedia.org/wiki/Boutisme).
>
> Defuse Security a créé [un outil pratique](https://defuse.ca/online-x86-assembler.htm) pour s'amuser avec la traduction entre assembleur et langage machine.

La RAM est la mémoire de votre ordinateur, un grand espace multi-usage qui stocke toutes les données utilisées par les programmes qui tournent dessus. Ça comprend le code des programmes en question, mais aussi le code au cœur de votre système d'exploitation. Le CPU lit toujours le code machine directement dans la RAM, et du code ne peut pas être exécuté s'il n'est pas chargé dans la RAM.

Le CPU stocke un *pointeur d'instruction* qui pointe vers l'emplacement en RAM où il va aller chercher (*fetch* en anglais) la prochaine instruction. Après avoir exécuté une instruction, le CPU bouge le pointeur et recommence. On appelle ça le *cycle d'instruction* (*fetch-execute cycle* en anglais).

<img src='/images/fetch-execute-cycle.png' loading='lazy' style='max-width: 360px; margin: 0 auto;' alt='Un diagramme expliquant le cycle d''instruction. Il y a deux bulles de texte. La première est légendée "Fetch" et contient le texte "Lire l''instruction en mémoire à l''emplacement du pointeur d''instruction". La seconde est légendée "Execute" et contient le texte "Exécuter l''instruction, et déplacer le pointeur d''instruction". La bulle "Fetch" a une flèche qui pointe vers la bulle "Execute", et la bulle "Execute" a une flèche qui re-pointe vers la bulle "Fetch", induisant un processus répétitif' width='848' height='458' />

Après avoir exécuté une instruction, le pointeur se déplace juste après l'instruction en mémoire, afin de pointer vers l'instruction suivante. C'est pour ça que le tourne ! Le pointeur d'instruction ne fait qu'avancer, exécutant du code machine dans l'ordre dans lequel il a été stocké en mémoire. Certaines instructions peuvent dire au pointeur de sauter ailleurs, ou à différents endroits selon une condition; cela permet de ré-utiliser du code ou de mettre en place une logique conditionnelle.

Ce pointeur d'instruction est stocké dans un [*registre*](https://fr.wikipedia.org/wiki/Registre_de_processeur). Les registres sont de petits espaces de stockage dans lesquels le CPU peut très rapidement lire et écrire. Chaque architecture CPU a un nombre défini de registres, utilisés pour un tas de choses allant du stockage de valeurs temporaires pendant un calcul à la configuration du processeur.

Certains de ces registres peuvent être lus directement depuis du code machine, comme `ebx` dans notre diagramme précédent.

D'autres registres sont uniquement utilisés en interne par le CPU, mais peuvent souvent être mis à jour ou lus en utilisant des instructions spéciales. On peut par exemple citer le pointeur d'instruction, qui ne peut pas être directement lu mais peut être mis a jour par exemple par une instruction de saut.

## Les processeurs sont naïfs

Revenons-en à notre question de base : qu'est-ce qui se passe lorsqu'on exécute un programme sur son ordinateur ? D'abord, il se passe des trucs pour se préparer à l’exécuter - on y reviendra - mais à la fin de ce processus on a du code machine dans un fichier quelque part. Le système d'exploitation le charge quelque part en mémoire et demande au CPU de déplacer le pointeur d'instruction de ce déplacer à cet endroit dans la RAM. Le processeur continue alors son cycle d'instruction, le programme commence à s’exécuter!

(C'était un des moments prise de tête pour moi — sérieux, c'est comme ça que le programme que vous utilisez pour lire cet article tourne ! Votre processeur récupère les instructions de votre navigateur dans la RAM à la suite et les exécute, et votre navigateur affiche cet article.)

<img src='/images/instruction-pointer.png' loading='lazy' style='max-width: 400px;' alt='Un diagramme montrant une série d''octets de code machine en RAM. Un octet mis en valeur est indiqué par une flèche légendée "Pointeur d''instruction", et des flèches montrent comment le pointeur d''instruction se déplace en mémoire' width='935' height='372' />

Il se trouve que les CPU ont une vue super basique de leur environnement; ils ne voient que le pointeur d'instructions et un peu de leur état interne. Les processus (*process* en anglais) sont de pures abstractions du système d'exploitation, et non pas quelque chose que les CPU peuvent comprendre ou suivre.

*\*grands gestes\* les process sont des abstractions créées par les ~~développeurs de systèmes d'exploitations~~ grosses boites d'informatique pour vendre plus de machines*

Pour moi, ça pose plus de questions que ça n'en résous :

1. Si le CPU ne connait pas le concept de multiprocess et se contente d’exécuter des instruction de façon séquentielle, comment il fait pour ne pas rester coincé dans un programme ? Comment plusieurs programmes peuvent s’exécuter en même temps ?
2. Si les programmes s’exécutent directement sur le CPU, et si le CPU peut directement accéder à la RAM, pourquoi du code ne peut pas accéder à la zone mémoire d’un autre programme, ou dieu nous en garde, au kernel?
3. Et au fait, c’est quoi le mécanisme qui empêche les process d’exécuter n’importe-quoi et de faire ce qu’ils veulent à votre ordinateur? ET C’EST QUOI UN PUTAIN DE SYSCALL?

La question sur la mémoire mérite sa propre section, et est traitée dans le [chapitre 5](/the-translator-in-your-computer) — en gros, la plupart des accès mémoire passent par une couche de redirection qui réaffecte la totalité de l'espace d'adressage. Pour l'instant, on va faire semblant de croire que les programmes ont accès à toute la RAM, et que les ordinateurs ne peuvent exécuter qu'un programme à la fois. On reviendra sur ces deux suppositions en temps venu.

C'est l'heure de sauter dans notre premier puits sans fond pour arriver dans un mode plein de syscalls et de couches de sécurité.

> **Note : au fait, c'est quoi un kernel ?**
> 
> Your computer's operating system, like macOS, Windows, or Linux, is the collection of software that runs on your computer and makes all the basic stuff work. "Basic stuff" is a really general term, and so is "operating system" — depending on who you ask, it can include such things as the apps, fonts, and icons that come with your computer by default.
> 
> The kernel, however, is the core of the operating system. When you boot up your computer, the instruction pointer starts at a program somewhere. That program is the kernel. The kernel has near-full access to your computer's memory, peripherals, and other resources, and is in charge of running software installed on your computer (known as userland programs). We'll learn about how the kernel has this access — and how userland programs don't — over the course of this article.
>
> Linux is just a kernel and needs plenty of userland software like shells and display servers to be usable. The kernel in macOS is called [XNU](https://en.wikipedia.org/wiki/XNU) and is Unix-like, and the modern Windows kernel is called the [NT Kernel](https://en.wikipedia.org/wiki/Architecture_of_Windows_NT).

## Two Rings to Rule Them All

The *mode* (sometimes called privilege level or ring) a processor is in controls what it's allowed to do. Modern architectures have at least two options: kernel/supervisor mode and user mode. While an architecture might support more than two modes, only kernel mode and user mode are commonly used these days.

In kernel mode, anything goes: the CPU is allowed to execute any supported instruction and access any memory. In user mode, only a subset of instructions is allowed, I/O and memory access is limited, and many CPU settings are locked. Generally, the kernel and drivers run in kernel mode while applications run in user mode.

Processors start in kernel mode. Before executing a program, the kernel initiates the switch to user mode.

<img src='/images/kernel-mode-vs-user-mode.png' loading='lazy' style='max-width: 500px; margin: 0 auto;' alt='Two fake iMessage screenshots demonstrating the different between user and kernel mode protections. The first, labeled Kernel Mode: right side says "Read this protected memory!", left side replies "Here you go, dear :)". The second, labeled User Mode: right side says "Read this protected memory!", left side replies "No! Segmentation fault!"' width='1072' height='433' />

An example of how processor modes manifest in a real architecture: on x86-64, the current privilege level (CPL) can be read from a register called `cs` (code segment). Specifically, the CPL is contained in the two [least significant bits](https://en.wikipedia.org/wiki/Bit_numbering) of the `cs` register. Those two bits can store x86-64's four possible rings: ring 0 is kernel mode and ring 3 is user mode. Rings 1 and 2 are designed for running drivers but are only used by a handful of older niche operating systems. If the CPL bits are `11`, for example, the CPU is running in ring 3: user mode.
 
## What Even is a Syscall?

Programs run in user mode because they can't be trusted with full access to the computer. User mode does its job, preventing access to most of the computer — but programs need to be able to access I/O, allocate memory, and interact with the operating system *somehow*! To do so, software running in user mode has to ask the operating system kernel for help. The OS can then implement its own security protections to prevent programs from doing anything malicious.

If you've ever written code that interacts with the OS, you'll probably recognize functions like `open`, `read`, `fork`, and `exit`. Below a couple of layers of abstraction, these functions all use *system calls* to ask the OS for help. A system call is a special procedure that lets a program start a transition from user space to kernel space, jumping from the program's code into OS code.

User space to kernel space control transfers are accomplished using a processor feature called [*software interrupts*](https://en.wikipedia.org/wiki/Interrupt#Software_interrupts):

1. During the boot process, the operating system stores a table called an [*interrupt vector table*](https://en.wikipedia.org/wiki/Interrupt_vector_table) (IVT; x86-64 calls this the [interrupt descriptor table](https://en.wikipedia.org/wiki/Interrupt_descriptor_table)) in RAM and registers it with the CPU. The IVT maps interrupt numbers to handler code pointers.

  <img src='/images/interrupt-vector-table.png' loading='lazy' style='max-width: 300px; margin: 0 auto;' alt='A image of a table captioned "Interrupt Vector Table". The first column, labeled with a number sign, has a series of numbers starting at 01 and going to 04. The corresponding second column of the table, labeled "Handler Address", contains a random 8-byte-long hex number per entry. The bottom of the table has the text "So on and such forth..."' width='555' height='463' />

2. Then, userland programs can use an instruction like [INT](https://www.felixcloutier.com/x86/intn:into:int3:int1) which tells the processor to look up the given interrupt number in the IVT, switch to kernel mode, and then jump the instruction pointer to the memory address stored in the IVT.

When this kernel code finishes, it uses an instruction like [IRET](https://www.felixcloutier.com/x86/iret:iretd:iretq) to tell the CPU to switch back to user mode and return the instruction pointer to where it was when the interrupt was triggered.

(If you were curious, the interrupt ID used for system calls on Linux is `0x80`. You can read a list of Linux system calls on [Michael Kerrisk's online manpage directory](https://man7.org/linux/man-pages/man2/syscalls.2.html).)

### Wrapper APIs: Abstracting Away Interrupts

Here's what we know so far about system calls:

- User mode programs can't access I/O or memory directly. They have to ask the OS for help interacting with the outside world.
- Programs can delegate control to the OS with special machine code instructions like INT and IRET.
- Programs can't directly switch privilege levels; software interrupts are safe because the processor has been preconfigured *by the OS* with where in the OS code to jump to. The interrupt vector table can only be configured from kernel mode.

Programs need to pass data to the operating system when triggering a syscall; the OS needs to know which specific system call to execute alongside any data the syscall itself needs, for example, what filename to open. The mechanism for passing this data varies by operating system and architecture, but it's usually done by placing data in certain registers or on the stack before triggering the interrupt.

The variance in how system calls are called across devices means it would be wildly impractical for programmers to implement system calls themselves for every program. This would also mean operating systems couldn't change their interrupt handling for fear of breaking every program that was written to use the old system. Finally, we typically don't write programs in raw assembly anymore — programmers can't be expected to drop down to assembly any time they want to read a file or allocate memory.

<img src='/images/syscall-architecture-differences.png' loading='lazy' style='max-width: 650px; margin: 0 auto;' alt='A drawing captioned "System calls are implemented differently across architectures." On the left is a smiling CPU receiving some binary and spitting out a filename, file.txt. Separated on the right is a different CPU receiving the same binary data but with a confused and nauseous facial expression.' width='1057' height='360' />

So, operating systems provide an abstraction layer on top of these interrupts. Reusable higher-level library functions that wrap the necessary assembly instructions are provided by [libc](https://www.gnu.org/software/libc/) on Unix-like systems and part of a library called [ntdll.dll](https://learn.microsoft.com/en-us/windows-hardware/drivers/kernel/libraries-and-headers) on Windows. Calls to these library functions themselves don't cause switches to kernel mode, they're just standard function calls. Inside the libraries, assembly code does actually transfer control to the kernel, and is a lot more platform-dependent than the wrapping library subroutine.

When you call `exit(1)` from C running on a Unix-like system, that function is internally running machine code to trigger an interrupt, after placing the system call's opcode and arguments in the right registers/stack/whatever. Computers are so cool!

## The Need for Speed / Let's Get CISC-y

Many [CISC](https://en.wikipedia.org/wiki/Complex_instruction_set_computer) architectures like x86-64 contain instructions designed for system calls, created due to the prevalence of the system call paradigm.

Intel and AMD managed not to coordinate very well on x86-64; it actually has *two* sets of optimized system call instructions. [SYSCALL](https://www.felixcloutier.com/x86/syscall.html) and [SYSENTER](https://www.felixcloutier.com/x86/sysenter) are optimized alternatives to instructions like `INT 0x80`. Their corresponding return instructions, [SYSRET](https://www.felixcloutier.com/x86/sysret.html) and [SYSEXIT](https://www.felixcloutier.com/x86/sysexit), are designed to transition quickly back to user space and resume program code.

(AMD and Intel processors have slightly different compatibility with these instructions. `SYSCALL` is generally the best option for 64-bit programs, while `SYSENTER` has better support with 32-bit programs.)

Representative of the style, [RISC](https://en.wikipedia.org/wiki/Reduced_instruction_set_computer) architectures tend not to have such special instructions. AArch64, the RISC architecture Apple Silicon is based on, uses only [one interrupt instruction](https://developer.arm.com/documentation/ddi0596/2021-12/Base-Instructions/SVC--Supervisor-Call-) for syscalls and software interrupts alike. I think Mac users are doing fine&nbsp;:)

---

Whew, that was a lot! Let's do a brief recap:

- Processors execute instructions in an infinite fetch-execute loop and don't have any concept of operating systems or programs. The processor's mode, usually stored in a register, determines what instructions may be executed. Operating system code runs in kernel mode and switches to user mode to run programs.
- To run a binary, the operating system switches to user mode and points the processor to the code's entry point in RAM. Because they only have the privileges of user mode, programs that want to interact with the world need to jump to OS code for help. System calls are a standardized way for programs to switch from user mode to kernel mode and into OS code.
- Programs typically use these syscalls by calling shared library functions. These wrap machine code for either software interrupts or architecture-specific syscall instructions that transfer control to the OS kernel and switch rings. The kernel does its business and switches back to user mode and returns to the program code.

Let’s figure out how to answer my first question from earlier:

> If the CPU doesn't keep track of more than one process and just executes instruction after instruction, why doesn't it get stuck inside whatever program it's running? How can multiple programs run at once?

The answer to this, my dear friend, is also the answer to why Coldplay is so popular... clocks! (Well, technically timers. I just wanted to shoehorn that joke in.)

---
chapter: 2
title: Slice Dat Time
shortname: Multitasking
slug: slice-dat-time
updatedAt: 2023-08-02T18:14:02.296Z
---

Disons que vous créez un système d'exploitation, et vous voulez que vos utilisateurs soient capables de faire tourner plusieurs programmes en même temps. Ceci dit vous n'avez pas un super processeur multi-coeurs sous la main, donc votre CPU ne peut exécuter qu'une seule instruction à la fois !

Par chance, vous un développer de systèmes d'exploitations extrêmement futé. Vous comprenez que vous pouvez simuler du parallélisme en faisant faire des tours aux processus sur votre CPU. Si vous passez d'un processus à l'autre en exécutant quelques instructions de chaque, ils peuvent tous répondre sans que seul l'un d'entre eux monopolise le CPU.

Mais comment reprendre le contrôle depuis le code d'un programme pour changer de processus ? Après quelques recherches, vous découvrez que la plupart des ordinateurs disposent de puces timer. Vous pouvez programmer un timer pour déclencher un gestionnaire d'interruption après qu'un certain temps se soit écoulé.

## Interruptions matérielles

Plus tôt, on a parlé de comment les interruptions logicielles permettent de donner le contrôle d'un programme user-land au système d'exploitation. On appelle ça des interruptions "logicielles" parce qu’elles sont arbitrairement déclenchées par un programme — du code machine exécuté par le processeur dans son cycle d'exécution normal lui dit donner le contrôle au kernel.

<img src='/images/keyboard-hardware-interrupt.png' loading='eager' style='max-width: 500px; margin: 0 auto;' alt='Un dessin illustrant comment les interruptions matérielles interrompent le cycle d'exécution. En haut : a drawing of a keyboard with a highlighted key, with a lightning bolt drawn to a CPU on the right. On the bottom: some binary labeled "program code," a similar lightning bolt, and some more binary labeled "kernel code." The lightning bolt is labeled "interrupt triggers context switch."' width='935' height='503' />

Les schedulers des systèmes d'exploitation utilisent des *puces timer* comme les [PITs](https://en.wikipedia.org/wiki/Programmable_interval_timer) pour déclencher des interruptions matérielles et faire du multi-tâches :

1. Avant de sauter vers le code du pogramme, le système d'exploitation configure un timer pour déclencher une interruption après un certain laps de temps
2. Le système d'exploitation bascule en mode utilisateur et saute à la prochaine instruction du programme
3. Quand le timer arrive à son terme, il déclenche une interruption logicielle pour basculer en mode kernel et saute au code du système d'exploitation
4. L'OS peut maintenant sauvegarder l'endroit où le programme s'est arrêté, en charger un nouveau et repéter ce processus.

On appelle ça du *multi-tâche préemptif*, et le fait d'interrompre un processus s'appelle la [*préemption*](https://fr.wikipedia.org/wiki/Multit%C3%A2che_pr%C3%A9emptif). Par exemple, si vous lisez cet article dans votre navigateur et écoutez de la musique sur la même machine, votre ordinateur est sûrement en train de suivre ce cycle des miliers de fois par seconde.

## Calcul de l’intervalle de temps

Un *intervalle de temps* est la durée que l'OS autorise à un processus pour s'exécuter avant de le préempter. La façon la plus simple de définir cet intervalle est de donner à chaque processus la même durée, par exemple autour des 10ms, et de faire défiler les tâches dans l'ordre. On appelle ça *l'ordonnancement round-robin à intervalles de temps fixes*.

> **Note: fun facts jargon!**
> 
> Vous saviez que les intervalles de temps sont souvent appelés des "quantums" ? Maintenant oui, et vous pouvez impressionner vos copains techos. Je pense que je mérite des applaudissements pour ne pas sortir le mot quantum à chaque phrase de cet article.
> 
> En parlant de jargon, le kernel Linux utilise le [jiffy](https://github.com/torvalds/linux/blob/22b8cc3e78f5448b4c5df00303817a9137cd663f/include/linux/jiffies.h) comme unité de mesure pour compter les ticks d'horloge à fréquence fixe. Parmi d'autres trucs, les jiffies sont utiles pour mesurer la durée d'un intervalle de temps. La fréquence d'un jiffy sur Linux est d'environ 1000 Hz, mais on peut changer ça en compilant le kernel.

On peut améliorer le concept de intervalles de temps réguliers en définissat une *latence cible* — le temps maximum idéel qu'il faudrait à un processus pour répondre. La latence cible est le temps qu'il faut à un processeur pour reprendre son exécution après avoir été préempté, en supposant un nombre raisonnable de processus. *C'est chaud à visualiser ! Mais pas de souci, un diagramme arrive bientôt.*

On calcule les intervalles de temps en divisant la latence cible par le nombre total de tâches; c'est mieux qu'un intervalle fixe car ça élimine du des changements de tâche inutiles quand on a peu de tâches. Avec une latence cible de 15ms et 10 processus, chaque processus aurait 15/10, ou 1.5ms pour s'exécuter. Avec seulement trois processus, chacun aura une tranche plus longue (5ms), tout en respectant toujours la latence cible !

Le fait de changer de process est coûteux en performance parce qu'il nécessite de sauvegarder l'état entier d'un programme et d'en restaurer un autre. Après un certain point, un intervalle de temps trop petit peut causer des problèmes de performance à cause de problèmes changeant trop rapidement. Il est commun de donner à l'intervalle de temps une durée plancher (*granularité minimum*). Ca signifie que la latence cible est dépassée lorsque suffisamment de processus sont présents pour que la granularité minimale puisse prendre effet.

Au moment où j'écris cet article, le scheduler de Linux utilise une latence cible de 6ms et une granularité minimum de 0.75ms.

<img src='/images/linux-scheduler-target-latency.png' loading='lazy' style='max-width: 500px; margin: 0 auto;' alt='A diagram titled "Naive Dynamic Timeslice Round-Robin Scheduling." It depicts a time series of 3 different processes getting time to execute in a repeated cycle. In between the execution blocks of each process is a much shorter block labeled "kernel scheduler." The length of each program execution block is labeled "timeslice (2ms)." The distance from the start of process 1 executing to the next start of process 1 executing, encompassing the execution time of processes 2 and 3, is labeled as "target latency (6ms)."' width='935' height='433' />

L'ordonnancement round-robin utilisant cette façon de calculer les intervalles de temps est proche de ce que la plupart des ordinateurs font de nos jours. C'est toujours un peu naïf, la plupart des OS ont des schedulers plus complexes qui prennent en compte les priorités de processus et les deadlines. Depuis 2007, Linux utilise un scheduler qui s'appelle [Completely Fair Scheduler](https://docs.kernel.org/scheduler/sched-design-CFS.html). CFS fait tout un tas de trucs de science des ordinateurs pour prioriser les tâches et diviser le temps du CPU.

A chaque fois que l'OS préempte un processus, il doit charger le contexte d'exécution du nouveau programme qu'il avait sauvegardé, comprenant son environnement en mémoire. On fait ça en disant au CPU d'utiliser une autre *page table*, c'est à dire la correspondance entre les adresses physiques et virtuelles. C'est aussi le système qui empêche les programmes d'accéder à la mémoire d'autres programmes; on sautera dans ce trou de lapin aux chapitres [5](/the-translator-in-your-computer) et [6](/lets-talk-about-forks-and-cows) de cet artile.

## Note #1: Preemptabilité du kernel

Jusqu'ici, on a seulement parlé de la préemption et du scheduling des processus userland. Le kernel pourrait donner l'impression que les programmes laggent s'il prend trop longtemps à gérer un appel système ou exécuter le code d'un driver.

Les kernels modernes, Linux y-compris, sont des [kernels préemptifs](https://en.wikipedia.org/wiki/Kernel_preemption). Ca veut dire qu'ils sont fait de façon à ce que le code du kernel lui-même puisse être interrompu et schedulé comme des processus userland.

C'est pas très important à savoir à moins que vous écriviez un kernel, mais en gros chaque article que j'ai lu l'a mentionné, donc je me suis dit que j'allais le faire aussi ! Des connaissances en plus font rarement du mal.

## Note #2: Une leçon d'histoire

Les vieux systèmes d'exploitation, ce qui inclue le Mac OS classique et les versions de Windows bien avant NT, utilisaient un prédécesseur sur multi-tâche préemptif. Plutôt que ce soit l'OS qui décidé de quand préempter un programme, les programmes eux-mêmes décidaient de laisser la main à l'OS. Ils déclenchaient une interruption logicielle pour dire "hey, tu peux laisser un autre programme tourner maintenant". Ces demandes explicitent étaient la seule façon pour l'OS de reprendre le contrôle et de passer au prochain processus sur la liste.

On appelle ça le [*multitâche coopératif*](https://en.wikipedia.org/wiki/Cooperative_multitasking). Il possède un certain nombre de défauts : des programmes malicieux ou juste mal pensés pouvaient facilement freezer l'OS tout entier, et il est presque impossible d'assurer une consistance temporelle pour des tâches temps-réel et/ou sensibles. Pour ces raisons, le monde de la tech a basculé sur du multitâche préemptif il y a longtemps et n'a jamais changé.
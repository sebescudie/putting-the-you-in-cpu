---
chapter: 2
title: Slice Dat Time
shortname: Multitasking
slug: slice-dat-time
updatedAt: 2023-08-02T18:14:02.296Z
---

Disons que vous créez un système d'exploitation, et vous voulez que vos utilisateurs soient capables de faire tourner plusieurs programmes en même temps. Ceci dit vous n'avez pas un super processeur multi-coeurs sous la main, donc votre CPU ne peut exécuter qu'une seule instruction à la fois !

Par chance, vous un développer de systèmes d'exploitations extrêmement futé. Vous comprenez que vous pouvez simuler du parallélisme en faisant faire des tours aux processus sur votre CPU. Si vous passez d'un processus à l'autre en exécutant quelques instructions de chaque, ils peuvent tous répondre sans que seul l'un d'entre eux monopolise le CPU.

Mais comment reprendre le contrôle depuis le code d'un programme pour changer de processus ? Après quelques recherches, vous découvrez que la plupart des ordinateurs disposent de puces timer. Vous pouvez programmer un timer pour déclencher un gestionnaire d'interruption après qu'un certain temps se soit écoulé.

## Interruptions matérielles

Plus tôt, on a parlé de comment les interruptions logicielles permettent de donner le contrôle d'un programme user-land au système d'exploitation. On appelle ça des interruptions "logicielles" parce qu’elles sont arbitrairement déclenchées par un programme — du code machine exécuté par le processeur dans son cycle d'exécution normal lui dit donner le contrôle au kernel.

<img src='/images/keyboard-hardware-interrupt.png' loading='eager' style='max-width: 500px; margin: 0 auto;' alt='Un dessin illustrant comment les interruptions matérielles interrompent le cycle d'exécution. En haut : a drawing of a keyboard with a highlighted key, with a lightning bolt drawn to a CPU on the right. On the bottom: some binary labeled "program code," a similar lightning bolt, and some more binary labeled "kernel code." The lightning bolt is labeled "interrupt triggers context switch."' width='935' height='503' />

Les schedulers des systèmes d'exploitation utilisent des *puces timer* comme les [PITs](https://en.wikipedia.org/wiki/Programmable_interval_timer) pour déclencher des interruptions matérielles et faire du multi-tâches :

1. Avant de sauter vers le code du pogramme, le système d'exploitation configure un timer pour déclencher une interruption après un certain laps de temps
2. Le système d'exploitation bascule en mode utilisateur et saute à la prochaine instruction du programme
3. Quand le timer arrive à son terme, il déclenche une interruption logicielle pour basculer en mode kernel et saute au code du système d'exploitation
4. L'OS peut maintenant sauvegarder l'endroit où le programme s'est arrêté, en charger un nouveau et repéter ce processus.

On appelle ça du *multi-tâche préemptif*, et le fait d'interrompre un processus s'appelle la [*préemption*](https://fr.wikipedia.org/wiki/Multit%C3%A2che_pr%C3%A9emptif). Par exemple, si vous lisez cet article dans votre navigateur et écoutez de la musique sur la même machine, votre ordinateur est sûrement en train de suivre ce cycle des miliers de fois par seconde.

## Calcul de l’intervalle de temps

A *timeslice* is the duration an OS scheduler allows a process to run before preempting it. The simplest way to pick timeslices is to give every process the same timeslice, perhaps in the 10&nbsp;ms range, and cycle through tasks in order. This is called *fixed timeslice round-robin* scheduling.

Un *intervalle de temps* est la durée que l'OS autorise à un processus pour s'exécuter avant de le préempter. La façon la plus simple de définir cet intervalle est de donner à chaque processus la même durée, par exemple autour des 10ms, et de faire défiler les tâches dans l'ordre. On appelle ça *l'ordonnancement round-robin à intervalles de temps fixes*.

> **Note: fun facts jargon!**
> 
> Vous saviez que les intervalles de temps sont souvent appelés des "quantums" ? Maintenant oui, et vous pouvez impressionner vos copains techos. Je pense que je mérite des applaudissements pour ne pas sortir le mot quantum à chaque phrase de cet article.
> 
> En parlant de jargon, le kernel Linux utilise le [jiffy](https://github.com/torvalds/linux/blob/22b8cc3e78f5448b4c5df00303817a9137cd663f/include/linux/jiffies.h) comme unité de mesure pour compter les ticks d'horloge à fréquence fixe. Parmi d'autres trucs, les jiffies sont utiles pour mesurer la durée d'un intervalle de temps. La fréquence d'un jiffy sur Linux est d'environ 1000 Hz, mais on peut changer ça en compilant le kernel.

A slight improvement to fixed timeslice scheduling is to pick a *target latency* — the ideal longest time for a process to respond. The target latency is the time it takes for a process to resume execution after being preempted, assuming a reasonable number of processes. *This is pretty hard to visualize! Don't worry, a diagram is coming soon.*

Timeslices are calculated by dividing the target latency by the total number of tasks; this is better than fixed timeslice scheduling because it eliminates wasteful task switching with fewer processes. With a target latency of 15&nbsp;ms and 10 processes, each process would get 15/10 or 1.5&nbsp;ms to run. With only 3 processes, each process gets a longer 5&nbsp;ms timeslice while still hitting the target latency.

Process switching is computationally expensive because it requires saving the entire state of the current program and restoring a different one. Past a certain point, too small a timeslice can result in performance problems with processes switching too rapidly. It's common to give the timeslice duration a lower bound (*minimum granularity*). This does mean that the target latency is exceeded when there are enough processes for the minimum granularity to take effect.

At the time of writing this article, Linux's scheduler uses a target latency of 6&nbsp;ms and a minimum granularity of 0.75&nbsp;ms.

<img src='/images/linux-scheduler-target-latency.png' loading='lazy' style='max-width: 500px; margin: 0 auto;' alt='A diagram titled "Naive Dynamic Timeslice Round-Robin Scheduling." It depicts a time series of 3 different processes getting time to execute in a repeated cycle. In between the execution blocks of each process is a much shorter block labeled "kernel scheduler." The length of each program execution block is labeled "timeslice (2ms)." The distance from the start of process 1 executing to the next start of process 1 executing, encompassing the execution time of processes 2 and 3, is labeled as "target latency (6ms)."' width='935' height='433' />

Round-robin scheduling with this basic timeslice calculation is close to what most computers do nowadays. It's still a bit naive; most operating systems tend to have more complex schedulers which take process priorities and deadlines into account. Since 2007, Linux has used a scheduler called [Completely Fair Scheduler](https://docs.kernel.org/scheduler/sched-design-CFS.html). CFS does a bunch of very fancy computer science things to prioritize tasks and divvy up CPU time.

Every time the OS preempts a process it needs to load the new program's saved execution context, including its memory environment. This is accomplished by telling the CPU to use a different *page table*, the mapping from "virtual" to physical addresses. This is also the system that prevents programs from accessing each other's memory; we'll go down this rabbit hole in chapters [5](/the-translator-in-your-computer) and [6](/lets-talk-about-forks-and-cows) of this article.

## Note #1: Kernel Preemptability

So far, we've been only talking about the preemption and scheduling of userland processes. Kernel code might make programs feel laggy if it took too long handling a syscall or executing driver code.

Modern kernels, including Linux, are [preemptive kernels](https://en.wikipedia.org/wiki/Kernel_preemption). This means they're programmed in a way that allows kernel code itself to be interrupted and scheduled just like userland processes.

This isn't very important to know about unless you're writing a kernel or something, but basically every article I've read has mentioned it so I thought I would too! Extra knowledge is rarely a bad thing.

## Note #2: A History Lesson

Ancient operating systems, including classic Mac OS and versions of Windows long before NT, used a predecessor to preemptive multitasking. Rather than the OS deciding when to preempt programs, the programs themselves would choose to yield to the OS. They would trigger a software interrupt to say, "hey, you can let another program run now." These explicit yields were the only way for the OS to regain control and switch to the next scheduled process.

This is called [*cooperative multitasking*](https://en.wikipedia.org/wiki/Cooperative_multitasking). It has a couple major flaws: malicious or just poorly designed programs can easily freeze the entire operating system, and it's nigh impossible to ensure temporal consistency for realtime/time-sensitive tasks. For these reasons, the tech world switched to preemptive multitasking a long time ago and never looked back.
